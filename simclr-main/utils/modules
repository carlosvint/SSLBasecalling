import numpy as np
import pandas as pd
import shutil, time, os, requests, random, copy

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as Data

from ctc.ctc_encoder import Encoder
import generate_dataset.constants as constants


class Residual(nn.Module):
    """The Residual block of ResNet."""
    def __init__(self, input_channels, num_channels,):
        super().__init__()

        self.strides = 1
        self.dilation = 1
        
        self.conv1 = nn.Conv1d(input_channels, num_channels,
                               kernel_size=1, padding='same', stride=self.strides,
                               dilation=self.dilation, bias=False)
        self.conv2 = nn.Conv1d(num_channels, num_channels,
                               kernel_size=3, padding='same', stride=self.strides,
                               dilation=self.dilation, bias=False)
        self.conv3 = nn.Conv1d(num_channels,num_channels,
                               kernel_size=1, padding='same', stride=self.strides,
                               dilation=self.dilation, bias=False)


        self.bn1 = nn.BatchNorm1d(num_channels)
        self.bn2 = nn.BatchNorm1d(num_channels)
        self.bn3 = nn.BatchNorm1d(num_channels)

        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv3(out)
        out = self.bn3(out)
        out += residual
        out = self.relu(out)
        return out


class w2v2ProjectionHead(nn.Module):
    def __init__(self,
                 in_features,
                 out_features):
        super(w2v2ProjectionHead,self).__init__()
        """Wav2Vec2FeatureProjection(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (projection): Linear(in_features=512, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)"""

        self.in_features = in_features
        self.out_features = out_features

        self.layer_norm = nn.LayerNorm((in_features), eps=1e-05, elementwise_affine=True)
        self.projection = nn.Linear(in_features=in_features, out_features=out_features, bias=True)
        self.dropout = nn.Dropout(p=0.1, inplace=False)

    def forward(self,x):
        out = self.layer_norm(x)
        out1 = self.dropout(self.projection(out)) 

        return out1


class SimCLRProjectionHead(nn.Module):
    def __init__(self,
                 in_features,
                 hidden_size,
                 out_features):
        super(SimCLRProjectionHead,self).__init__()
        self.in_features = in_features
        self.hidden_size = hidden_size
        self.out_features = out_features


        self.projection = nn.Sequential(nn.Linear(in_features, hidden_size, bias=False), nn.BatchNorm1d(hidden_size),
                               nn.ReLU(inplace=True), nn.Linear(hidden_size, out_features, bias=True))


    def forward(self,x):
        out = self.projection(x)
        
        return out


class BasecallProjectionHead(nn.Module):
    def __init__(self, in_features,
                 hidden_size,
                 out_features):
        super(BasecallProjectionHead,self).__init__()
        self.in_features = in_features
        self.hidden_size = hidden_size
        self.out_features = out_features
        self.pooling = nn.AvgPool1d(512)
        
        
        self.projection = nn.Sequential(nn.Linear(in_features, hidden_size, bias=False),
                        nn.ReLU(inplace=True), nn.Linear(hidden_size, out_features, bias=True))


    def forward(self,x):
        out = self.pooling(x.transpose(-1,-2))
        out = self.projection(out.squeeze())
        
        return out


class ChannelMixer(nn.Module):
    def __init__(self, in_features,
                 hidden_size,
                 out_features):
        super(ChannelMixer,self).__init__()
        self.in_features = in_features
        self.hidden_size = hidden_size
        self.out_features = out_features
        self.pooling = nn.MaxPool1d(256)

        self.channel_mixer = nn.Linear(256, 1)

        self.projection = nn.Sequential(nn.Linear(in_features, hidden_size, bias=False),
                nn.ReLU(inplace=True), nn.Linear(hidden_size, out_features, bias=True)) 

    def forward(self,x):
        out = self.channel_mixer(x)
        out = self.projection(out.transpose(-1,-2))
        
        return out.transpose(-1,-2)
    

class FeatureExtractor(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(FeatureExtractor, self).__init__()

        self.padding = 1
        self.kernel_size = 3
        self.stride = 2
        self.dilation = 1

        self.feature_extractor = nn.Sequential(
            nn.Conv1d(in_channels=in_channels, out_channels= out_channels//2, kernel_size=self.kernel_size, 
                        stride=self.stride, padding=self.padding, dilation=self.dilation, bias=False),
            nn.BatchNorm1d(num_features=out_channels//2),
            nn.ReLU(inplace=True),
            
            nn.Conv1d(in_channels=out_channels//2, out_channels=out_channels, kernel_size=self.kernel_size,
                        stride=self.stride, padding=self.padding, dilation=self.dilation, bias=False),
            nn.BatchNorm1d(num_features=out_channels),
            nn.ReLU(inplace=True))

    def forward(self, signal):
        x = signal.transpose(-1, -2)
        h = self.feature_extractor(x)

        return h.transpose(-1,-2)


class ResidualFeatureExtractor(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(ResidualFeatureExtractor, self).__init__()

        self.padding = 1
        self.kernel_size = 3
        self.stride = 2
        self.dilation = 1
        d_model = out_channels

        self.feature_extractor = nn.Sequential(nn.Conv1d(in_channels,d_model, 1), Residual(d_model,d_model), 
                                        Residual(d_model,d_model), Residual(d_model,d_model), nn.MaxPool1d(4), Residual(d_model,d_model))

    def forward(self, signal):
        x = signal.transpose(-1, -2)
        h = self.feature_extractor(x)

        return h.transpose(-1,-2)

class SacallFeatureExtractor(nn.Module):
    def __init__(self):
        super(SacallFeatureExtractor, self).__init__()

        self.d_model = 256
        self.d_ff = 1024
        self.n_head = 8
        self.n_layers = 6
        self.dropout = 0.1

        self.feature_extractor = Encoder(d_model=self.d_model,
                                   d_ff=self.d_ff,
                                   n_head=self.n_head,
                                   num_encoder_layers=self.n_layers,
                                   dropout=self.dropout)

    def forward(self, signal):
        signal_lengths = signal.squeeze(2).ne(constants.SIG_PAD).sum(1)
        enc_output, enc_output_lengths = self.feature_extractor(
            signal, signal_lengths)

        return enc_output, enc_output_lengths

class PreModel(nn.Module):
    def __init__(self, feature_extractor='sacall', projection_head='basecall'):
        super().__init__()
        self.projection_head = projection_head
        self.feature_extractor = feature_extractor
        
        #PRETRAINED MODEL
        if self.feature_extractor == 'sacall':
            self.pretrained = FeatureExtractor(1, 256)
        if self.feature_extractor == 'residual':
            self.pretrained = ResidualFeatureExtractor(1,256)
        if self.feature_extractor == 'transformer':
            self.pretrained = SacallFeatureExtractor()
        
        #self.pretrained.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)
        #self.pretrained.maxpool = Identity()
        #self.pretrained.fc = Identity()
        
        for p in self.pretrained.parameters():
            p.requires_grad = True
        
        if projection_head == 'simclr':
            self.projector = SimCLRProjectionHead(256, 512, 128)
        if projection_head == 'wav2vec2':
            self.projector = w2v2ProjectionHead(256, 768)
        if projection_head == 'basecall':
            self.projector = BasecallProjectionHead(256,128,128)
        if projection_head == 'mixer':
            self.projector = ChannelMixer(512,128,128)

    def forward(self,x):
        if self.feature_extractor == 'transformer':
            h, _ = self.pretrained(x)
            z = self.projector(h)
        else:
            h = self.pretrained(x)
            z = self.projector(h)
        
        return z


